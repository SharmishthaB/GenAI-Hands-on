{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b42cb12-2026-4f49-8de3-042eaa8394fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f3595e-f6bb-45dd-a8eb-1cec966e2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dcc0865-ebc6-46ba-b620-8d13dc7d3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d558c5-8910-4cb8-8884-86dfece18620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sharm\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Output: [{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bert_generator = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
    "    bert_output = bert_generator(prompt, max_length=30)\n",
    "    print(\"BERT Output:\", bert_output)\n",
    "except Exception as e:\n",
    "    print(\"BERT Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c700971a-acdc-4d70-a7fd-5bb61f8a3963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Output: [{'generated_text': 'The future of Artificial Intelligence is'}]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    roberta_generator = pipeline(\"text-generation\", model=\"roberta-base\")\n",
    "    roberta_output = roberta_generator(prompt, max_length=30)\n",
    "    print(\"RoBERTa Output:\", roberta_output)\n",
    "except Exception as e:\n",
    "    print(\"RoBERTa Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9877e2a-cb99-4402-9bc2-e866be4739c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Output: [{'generated_text': 'The future of Artificial Intelligence isidesINGExampleINGExampleAdds heated refine refineDetroitExampleExample heated Planetary clinics Dis stretch Raysenforcement Rays misled misled groom threat scientist revamped heated heated heated refine Defeat Stanford clinics existsoomenforcementenforcement Eye Gamer clinics clinics pepp clinics pepp Eye Eyeive Eye clinics clinics clinics rebel threat Proof clinics Eye Dong clinics clinics 1932 Eye Donn voters process Defeat clinics clinics pavement clinics clinics Kus clinics clinics reperc surf clinics clinics carniv clinics clinics tightly Ru Donn Donn Mirror clinics clinics blending Donn interception wasted clinics wastedergy wasted Mirror adjourn clinics blending Mirror milliseconds Donn process wasted clinics Def wasted Melt clinics wasted DN Featuring intelligent clinics process Eye Eye clinics adjourn Eye Eye Donnhour Ru Donn clinics clinics Donn Donn EyePD existsPD DonnPD intelligent Ru Allow adjourn blending rebel 1932 1932PD Eye Eye intelligent Ru Donn process adjourn Def clinics Eyehour RuApplic Mirror Donn Donn blending Ru Ru Ru Donn Mirror EyePDPD Ruhour charm Donn Eye intelligent Donn imitation Donn Donn Oslo interception blending Donn DonnPD interception intelligent Ru lapsOfficialPD Ru Donn Ru Donn lapsopoulos Donn Donn Donn Allow subpoenaPD Donnistedisted Donn Donn Ru imitation tightly EyePD EyeAND Stalin Eye imitation bartPD Ru OPECMove Donn Donn 1932 Donn 100 Donn charm tightlyisted DonnAND EyePD StalinGraham Stalin Donn Donn Stalin DonnGraham rebel charm Stalin interception Stalin Ru Ru Franks blending ACLU Stalin'}]\n"
     ]
    }
   ],
   "source": [
    "bart_generator = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
    "bart_output = bart_generator(prompt, max_length=40)\n",
    "print(\"BART Output:\", bart_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d6897-7611-4710-85df-cf0df74a707e",
   "metadata": {},
   "source": [
    "Experiment 2: Masked Language Modeling (Missing Word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22253d-8b7e-4a46-ba02-1560a9e1c816",
   "metadata": {},
   "source": [
    "defining model specific prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2bc7c47-d24c-43a2-8a0e-3f43ffbfcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_text = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "roberta_text = \"The goal of Generative AI is to <mask> new content.\"\n",
    "bart_text = \"The goal of Generative AI is to <mask> new content.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8c4e1-2412-4765-a064-9015d5068a28",
   "metadata": {},
   "source": [
    "Run Fill-Mask Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8501b54-362e-4bf4-b964-e4c82961b43d",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "331d8d31-6842-4257-9b90-f698465192e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create : 0.5397\n",
      "generate : 0.1558\n",
      "produce : 0.0541\n",
      "develop : 0.0445\n",
      "add : 0.0176\n"
     ]
    }
   ],
   "source": [
    "bert_fm = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "bert_preds = bert_fm(bert_text)\n",
    "\n",
    "for p in bert_preds[:5]:\n",
    "    print(p[\"token_str\"], \":\", round(p[\"score\"], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0d5b3-5c2a-469a-af78-2d41c085d025",
   "metadata": {},
   "source": [
    "RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c19799-de77-4300-ae73-39e0d56d4c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " generate : 0.3711\n",
      " create : 0.3677\n",
      " discover : 0.0835\n",
      " find : 0.0213\n",
      " provide : 0.0165\n"
     ]
    }
   ],
   "source": [
    "roberta_fm = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
    "roberta_preds = roberta_fm(roberta_text)\n",
    "\n",
    "for p in roberta_preds[:5]:\n",
    "    print(p[\"token_str\"], \":\", round(p[\"score\"], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76849656-bfbe-4a1d-a21e-c9a5fd3d6d81",
   "metadata": {},
   "source": [
    "BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06284a62-b667-40c8-94d8-fb563bd9bfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create : 0.0746\n",
      " help : 0.0657\n",
      " provide : 0.0609\n",
      " enable : 0.0359\n",
      " improve : 0.0332\n"
     ]
    }
   ],
   "source": [
    "bart_fm = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
    "bart_preds = bart_fm(bart_text)\n",
    "\n",
    "for p in bart_preds[:5]:\n",
    "    print(p[\"token_str\"], \":\", round(p[\"score\"], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db246af6-491e-4859-b944-c8371dd5f54d",
   "metadata": {},
   "source": [
    "Experiment 3: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6faddf6-bf5b-4eb5-9746-ab806235eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the risks?\"\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc1354c8-a52d-43e6-91fd-dfd1fbc828eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Answer: {'score': 0.007461190689355135, 'start': 46, 'end': 61, 'answer': 'hallucinations,'}\n"
     ]
    }
   ],
   "source": [
    "bert_qa = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
    "\n",
    "bert_answer = bert_qa(\n",
    "    question=question,\n",
    "    context=context\n",
    ")\n",
    "\n",
    "print(\"BERT Answer:\", bert_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fd2476f-d414-4309-b5a8-50d8e42e85ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Answer: {'score': 0.023195627378299832, 'start': 72, 'end': 81, 'answer': 'deepfakes'}\n"
     ]
    }
   ],
   "source": [
    "roberta_qa = pipeline(\"question-answering\", model=\"roberta-base\")\n",
    "\n",
    "roberta_answer = roberta_qa(\n",
    "    question=question,\n",
    "    context=context\n",
    ")\n",
    "\n",
    "print(\"RoBERTa Answer:\", roberta_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a15cd6a-645a-4498-ab86-91bf8c02dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Answer: {'score': 0.03156507387757301, 'start': 20, 'end': 67, 'answer': 'significant risks such as hallucinations, bias,'}\n"
     ]
    }
   ],
   "source": [
    "bart_qa = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
    "\n",
    "bart_answer = bart_qa(\n",
    "    question=question,\n",
    "    context=context\n",
    ")\n",
    "\n",
    "print(\"BART Answer:\", bart_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a684039-b51a-4b7d-bbef-d271545f7741",
   "metadata": {},
   "source": [
    "Observation Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9eee1-5df6-4e9c-99e0-70e6f780f4d3",
   "metadata": {},
   "source": [
    "| Task           | Model   | Classification (Success/Failure) | Observation (What actually happened?)                                                                     | Why did this happen? (Architectural Reason)                                                                                                                |\n",
    "| :------------- | :------ | :------------------------------- | :-------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Generation** | BERT    | *Failure*                        | The model produced a long sequence of dots instead of meaningful text.                                    | because BERT is an encoder-only model trained for masked token prediction, not autoregressive next-token generation.                                       |\n",
    "|                | RoBERTa | *Failure*                        | The output repeated the input prompt without generating any new tokens.                                   | RoBERTa is also encoder only and it lacks a decoder head for text generation.                                                                              |\n",
    "|                | BART    | *Success (Low quality)*          | The model generated a long continuation, but the text was noisy, repetitive, and semantically incoherent. | BART has an encoder decoder architecture with an autoregressive decoder,  allowing it to generate text even with minimal pretraining for open-ended tasks. |\n",
    "| **Fill-Mask**  | BERT    | *Success*                        | Predicted highly appropriate verbs such as “create” (0.54) and “generate” with strong confidence.         | BERT is trained using Masked Language Modeling (MLM), making it highly effective at predicting missing words.                                              |\n",
    "|                | RoBERTa | *Success*                        | Accurately predicted “generate” and “create” with balanced, high confidence scores.                       | RoBERTa is an optimized encoder only model trained extensively on MLM with more data and better tuning.                                                    |\n",
    "|                | BART    | *Partial Success*                | Predicted reasonable but less precise verbs with much lower confidence scores.                            | BART is trained as a denoising encoder-decoder model, it is not specifically optimized for single token MLM tasks.                                         |\n",
    "| **QA**         | BERT    | *Partial Success*                | Extracted only a single risk (“hallucinations”) with a very low confidence score.                         | BERT supports extractive QA, but the base model is not fine-tuned on QA datasets like SQuAD.                                                               |\n",
    "|                | RoBERTa | *Partial Success*                | Returned a different single risk (“deepfakes”), again with low confidence and incomplete coverage.        | RoBERTa can perform span extraction, but it does not have fine-tuning specifically for QA to choose full answer spans.                                     |\n",
    "|                | BART    | *Partial Success*                | Extracted a longer phrase covering multiple risks, but it still incomplete and low confidence.            | BART can handle sequences, but it is designed for generative seq2seq tasks instead of extractive QA.                                                       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
